# 系统架构说明（面试用）

## 🏗️ 整体架构

```
┌─────────────┐
│   用户请求   │
└──────┬──────┘
       │
       ▼
┌─────────────────┐
│   API 服务器     │  (Gin 框架)
│  /api/v1/query  │
└──────┬──────────┘
       │
       ▼
┌─────────────────┐
│   RAG 服务      │  (核心协调器)
└──────┬──────────┘
       │
       ├──────────┬──────────┬──────────┐
       ▼          ▼          ▼          ▼
┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐
│ Embedding│ │Retriever │ │  Ranker  │ │   LLM    │
│  嵌入    │ │  检索    │ │  排序    │ │  生成    │
└──────────┘ └──────────┘ └──────────┘ └──────────┘
```

## 📦 模块详解

### 1. Embedding 模块
**作用**: 文本 → 向量

```
输入: "Go 语言很强大"
输出: [0.1, 0.5, 0.3, ..., 0.2]  (128 维向量)
```

**实现**: `SimpleEmbedder`
- 使用词频统计
- 哈希映射到固定维度
- L2 归一化

**面试要点**:
- 为什么需要向量化？→ 计算机只能处理数字
- 为什么归一化？→ 统一向量长度，方便计算相似度

### 2. Retriever 模块
**作用**: 根据问题找最相关的文档

```
输入: 问题向量 + Top-K
输出: 最相关的 K 个文档
```

**实现**: `MemoryRetriever`
- 内存存储文档和向量
- 余弦相似度计算
- Top-K 检索

**面试要点**:
- 为什么用余弦相似度？→ 只关注方向，适合文本
- 为什么用内存？→ 简单快速，适合学习
- 生产环境用什么？→ 向量数据库（Qdrant、Pinecone）

### 3. Ranker 模块
**作用**: 对检索结果排序

```
输入: 检索结果列表
输出: 排序后的结果列表
```

**实现**: 
- `SimpleRanker`: 按分数排序
- `Reranker`: 支持多样性过滤
- `BM25Ranker`: BM25 算法

**面试要点**:
- 为什么需要排序？→ 提高结果质量
- 什么是重排序？→ 二次排序，考虑更多因素

### 4. Prompt 模块
**作用**: 构建 LLM 提示词

```
输入: 用户问题 + 检索到的文档
输出: 格式化的提示词
```

**模板示例**:
```
Context: [检索到的文档内容]

Question: [用户问题]

Answer:
```

**面试要点**:
- 为什么需要模板？→ 让 LLM 理解上下文
- 提示词工程的重要性？→ 直接影响回答质量

### 5. LLM 模块
**作用**: 生成回答

```
输入: 提示词
输出: 生成的回答
```

**实现**:
- `MockLLM`: 模拟实现（学习用）
- `OpenAI`: OpenAI API（生产用）

**面试要点**:
- 为什么用接口？→ 可以切换不同的 LLM
- 流式生成是什么？→ 逐字输出，提升用户体验

## 🔄 数据流（核心流程）

### 查询流程

```
1. 用户提问
   "什么是 Go 语言？"
         ↓
2. Embedding: 问题 → 向量
   [0.1, 0.5, ...]
         ↓
3. Retriever: 向量相似度搜索
   找到最相关的文档（Top-K）
         ↓
4. Prompt: 组合问题和文档
   "Context: Go 语言是...\nQuestion: 什么是 Go 语言？"
         ↓
5. LLM: 生成回答
   "Go 语言是 Google 开发的..."
         ↓
6. 返回给用户
```

### 添加文档流程

```
1. 接收文档
   {id: "doc1", content: "..."}
         ↓
2. Embedding: 文档 → 向量
         ↓
3. 存储到 Retriever
   (内存中的 map)
```

## 🎯 设计模式

### 1. 接口隔离
每个模块都定义接口，实现可以替换

```go
type Embedder interface {
    EmbedText(ctx context.Context, text string) ([]float32, error)
}
```

**好处**: 易于测试、易于扩展

### 2. 依赖注入
在 main.go 中组装所有组件

```go
ragService := rag.NewRAGService(
    embeddingService,
    retrieverService,
    llmService,
)
```

**好处**: 解耦、可测试

### 3. 服务层模式
每个模块都有 Service 层包装接口

**好处**: 统一接口、便于扩展

## 🚀 性能考虑

### 当前实现（学习版）
- ✅ 内存存储：快速、简单
- ❌ 不支持持久化
- ❌ 不支持大规模数据

### 生产环境改进
1. **向量数据库**: Qdrant、Pinecone、pgvector
2. **缓存**: Redis 缓存常用查询
3. **并发**: Go 的 goroutine 处理并发请求
4. **批处理**: 批量嵌入文档

## 📊 系统指标（面试可能问）

- **延迟**: 
  - 嵌入: ~10ms（内存）
  - 检索: ~5ms（小数据集）
  - LLM: ~100-1000ms（取决于 API）

- **准确率**:
  - 取决于嵌入质量和 LLM 质量
  - 简单实现: ~60-70%
  - 生产环境: ~80-90%

- **扩展性**:
  - 当前: 支持数百个文档
  - 改进后: 支持百万级文档

## 🔧 技术选型理由

| 技术 | 选择理由 |
|------|---------|
| Go | 高性能、并发友好、适合后端 |
| Gin | 轻量级、性能好、易用 |
| 内存存储 | 简单、快速原型、无依赖 |
| 接口设计 | 易于扩展、测试友好 |

## 💡 面试常见问题

### Q1: 为什么选择 Go？
**A**: Go 在并发处理上有优势，适合构建高性能的 RAG 服务。而且我是 Go 后端工程师，熟悉这个语言。

### Q2: 向量检索的原理？
**A**: 
1. 文本向量化（Embedding）
2. 计算查询向量和文档向量的余弦相似度
3. 返回相似度最高的 Top-K 个文档

### Q3: 如何优化检索效果？
**A**:
1. 使用更好的嵌入模型（如 OpenAI embeddings）
2. 使用向量数据库（支持近似最近邻搜索）
3. 重排序（Reranking）提高精度
4. 混合检索（向量 + 关键词）

### Q4: 如何扩展到生产环境？
**A**:
1. 替换内存存储为向量数据库
2. 集成真实 LLM API
3. 添加缓存层
4. 实现监控和日志
5. 支持批量处理
6. 添加错误处理和重试机制

### Q5: RAG 的局限性？
**A**:
1. 依赖文档质量
2. 检索可能不准确
3. 上下文长度限制
4. 实时性要求高的场景可能延迟较大

## 📝 项目亮点（面试强调）

1. ✅ **完整的 RAG 实现**: 从嵌入到生成全流程
2. ✅ **模块化设计**: 易于扩展和维护
3. ✅ **接口抽象**: 可以切换不同实现
4. ✅ **RESTful API**: 标准化的接口设计
5. ✅ **学习导向**: 代码清晰，便于理解

## 🎓 学习价值

这个项目帮助你理解：
- RAG 系统的完整流程
- 向量检索的原理
- LLM 集成方式
- Go 后端开发实践



